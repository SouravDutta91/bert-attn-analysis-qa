# Analysis of Attention in BERT for QA Explainability
We are using the SQuAD 2.0 dataset for this task.

## Dependencies

`pip install pytorch-pretrained-bert`

## Model

https://drive.google.com/file/d/1hktnjAJOdOwPxTK3R-KST9-kUQFYPusM/view?usp=sharing

## Run the script

Run the file `code/script.py`.

## Citation

If you are referring to this work in your research, please cite as:

```
@article{arkhangelskaia2019whatcha,
  title={Whatcha lookin'at? DeepLIFTing BERT's Attention in Question Answering},
  author={Arkhangelskaia, Ekaterina and Dutta, Sourav},
  journal={arXiv preprint arXiv:1910.06431},
  year={2019}
}
```
